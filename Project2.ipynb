{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a22dfe0",
   "metadata": {},
   "source": [
    "___\n",
    "# <font color= #d4b1e6> **Proyecto 2: Neural Style Transfer** </font>\n",
    "- <Strong> Nombre de los integrantes: </Strong>  <font color=\"blue\">`Sarah Lucía Beltrán, Priscila Cervantes Ramírez, Mónica Ibarra Herrera & Antonia Horburger` </font>\n",
    "- <Strong> Materia: </Strong>  <font color=\"blue\">`Aprendizaje Máquina` </font>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487239ff",
   "metadata": {},
   "source": [
    "*Librerías a utlizar*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "708183b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports comunes\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# TensorFlow / Keras imports (para la sección Keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2213a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57429207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Rutas\n",
    "CONTENT_DIR = Path('images/content')\n",
    "STYLE_DIR = Path('images/style')\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b2e9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers (PyTorch)\n",
    "IMG_SIZE = 512 # puede reducir si GPU limitada\n",
    "\n",
    "\n",
    "loader = transforms.Compose([\n",
    "transforms.Resize(IMG_SIZE),\n",
    "transforms.CenterCrop(IMG_SIZE),\n",
    "transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "unloader = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5310dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga imagen y la convierte a tensor normalizado\n",
    "from pyexpat import features\n",
    "from unittest import result\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "def load_image_pil(path, size=IMG_SIZE):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x[:3, :, :])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "\n",
    "# Convierte tensor a PIL Image\n",
    "def tensor_to_pil(tensor):\n",
    "    tensor = tensor.cpu().clone().squeeze(0)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    image = transforms.ToPILImage()(tensor)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Matriz de Gram para cálculo de estilo\n",
    "def gram_matrix(input):\n",
    "    b, c, h, w = input.size()\n",
    "    features = input.view(b * c, h * w)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(b * c * h * w)\n",
    "\n",
    "\n",
    "# Convierte tensor a PIL Image\n",
    "def tensor_to_pil(tensor):\n",
    "    tensor = tensor.cpu().clone().squeeze(0)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    image = transforms.ToPILImage()(tensor)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Matriz de Gram para cálculo de estilo\n",
    "def gram_matrix(input):\n",
    "    b, c, h, w = input.size()\n",
    "    features = input.view(b * c, h * w)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(b * c * h * w)\n",
    "\n",
    "\n",
    "# Función de preservación de color (crominancia)\n",
    "def preserve_color_np(content_tensor, stylized_tensor):\n",
    "    # convierte a numpy\n",
    "    content_np = content_tensor.cpu().numpy().squeeze().transpose(1,2,0)\n",
    "    stylized_np = stylized_tensor.cpu().numpy().squeeze().transpose(1,2,0)\n",
    "    # convierte a YCrCb\n",
    "    import cv2\n",
    "    content_ycrcb = cv2.cvtColor((content_np*255).astype(np.uint8), cv2.COLOR_RGB2YCrCb)\n",
    "    stylized_ycrcb = cv2.cvtColor((stylized_np*255).astype(np.uint8), cv2.COLOR_RGB2YCrCb)\n",
    "    stylized_ycrcb[:,:,1:] = content_ycrcb[:,:,1:]\n",
    "    result = cv2.cvtColor(stylized_ycrcb, cv2.COLOR_YCrCb2RGB)/255.0\n",
    "    return torch.tensor(result.transpose(2,0,1), device=DEVICE).unsqueeze(0).float()\n",
    "\n",
    "\n",
    "# Convierte imagen a tensor con normalización ImageNet\n",
    "def image_to_tensor(image):\n",
    "    # si ya es tensor lo retorna\n",
    "    if isinstance(image, torch.Tensor): return image\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3fd07",
   "metadata": {},
   "source": [
    "*Definición de pérdidas de contenido y estilo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68ce5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target, weight=1.0):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.loss = 0.0\n",
    "\n",
    "\n",
    "def forward(self, input):\n",
    "    self.loss = F.mse_loss(input * self.weight, self.target)\n",
    "    return input\n",
    "\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature, weight=1.0):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.target = gram_matrix(target_feature).detach() * self.weight\n",
    "        self.loss = 0.0\n",
    "\n",
    "\n",
    "def forward(self, input):\n",
    "    G = gram_matrix(input)\n",
    "    self.loss = F.mse_loss(G * self.weight, self.target)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedfdcf",
   "metadata": {},
   "source": [
    "*Construcción del modelo con pérdidas insertadas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5f06fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "    style_img, content_img,\n",
    "    content_layers=['conv_4'],\n",
    "    style_layers=['conv_1','conv_2','conv_3','conv_4','conv_5'],\n",
    "    content_weight=1e5, style_weight=1e10,\n",
    "    style_layer_weights=None):\n",
    "\n",
    "\n",
    "    class Normalization(nn.Module):\n",
    "        def __init__(self, mean, std):\n",
    "            super(Normalization, self).__init__()\n",
    "            self.mean = torch.tensor(mean).view(-1,1,1).to(DEVICE)\n",
    "            self.std = torch.tensor(std).view(-1,1,1).to(DEVICE)\n",
    "        def forward(self, img):\n",
    "            return (img - self.mean) / self.std\n",
    "\n",
    "    normalization = Normalization(normalization_mean, normalization_std)\n",
    "\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "\n",
    "    if style_layer_weights is None:\n",
    "        style_layer_weights = {name:1.0 for name in style_layers}\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d): i += 1; name=f'conv_{i}'\n",
    "        elif isinstance(layer, nn.ReLU): name=f'relu_{i}'; layer=nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d): name=f'pool_{i}'\n",
    "        elif isinstance(layer, nn.BatchNorm2d): name=f'bn_{i}'\n",
    "        else: name=f'layer_{i}'\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target, weight=content_weight)\n",
    "            model.add_module(f'content_loss_{i}', content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            w = style_layer_weights.get(name, 1.0)\n",
    "            style_loss = StyleLoss(target_feature, weight=style_weight * w)\n",
    "            model.add_module(f'style_loss_{i}', style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "\n",
    "    for j in range(len(model)-1, -1, -1):\n",
    "        if isinstance(model[j], ContentLoss) or isinstance(model[j], StyleLoss): break\n",
    "        model = model[:(j+1)]\n",
    "\n",
    "\n",
    "        return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33971634",
   "metadata": {},
   "source": [
    "*Función principal de transferencia de estilo PyTorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bfe7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer_pytorch(cnn, normalization_mean, normalization_std,\n",
    "    content_img, style_img, input_img,\n",
    "    num_steps=300, style_weight=1e6, content_weight=1e0,\n",
    "    tv_weight=1e-6, style_layer_weights=None,\n",
    "    print_steps=50):\n",
    "    content_layers = ['conv_4']\n",
    "    style_layers = ['conv_1','conv_2','conv_3','conv_4','conv_5']\n",
    "\n",
    "\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(\n",
    "        cnn, normalization_mean, normalization_std, style_img, content_img,\n",
    "        content_layers, style_layers, content_weight, style_weight, style_layer_weights)\n",
    "\n",
    "\n",
    "    input_param = nn.Parameter(input_img.data)\n",
    "    optimizer = optim.LBFGS([input_param])\n",
    "\n",
    "\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "        def closure():\n",
    "            with torch.no_grad(): input_param.clamp_(0,1)\n",
    "            optimizer.zero_grad()\n",
    "            model(input_param)\n",
    "            style_score = sum(sl.loss for sl in style_losses)\n",
    "            content_score = sum(cl.loss for cl in content_losses)\n",
    "            tv = tv_weight * (torch.sum(torch.abs(input_param[:,:,1:,:]-input_param[:,:,:-1,:])) + torch.sum(torch.abs(input_param[:,:,:,1:]-input_param[:,:,:,:-1])))\n",
    "            loss = style_score + content_score + tv\n",
    "            loss.backward()\n",
    "            run[0] += 1\n",
    "            if run[0]%print_steps==0:\n",
    "                print(f\"Iter {run[0]}: Style {style_score.item():.2e} Content {content_score.item():.2e} TV {tv.item():.2e}\")\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "\n",
    "\n",
    "    with torch.no_grad(): input_param.clamp_(0,1)\n",
    "    return input_param.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bde68",
   "metadata": {},
   "source": [
    "*Función envolvente para control completo (Contenido, Textura, Color)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9151657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylize_with_controls(content_path, style_path, out_path,\n",
    "        content_weight=1e5, style_weight=1e10,\n",
    "        tv_weight=1e-6, num_steps=200,\n",
    "        preserve_color=True, style_layer_weights=None,\n",
    "        init='content'):\n",
    "    content = image_to_tensor(load_image_pil(content_path, IMG_SIZE))\n",
    "    style = image_to_tensor(load_image_pil(style_path, IMG_SIZE))\n",
    "    input_img = content.clone() if init=='content' else torch.randn_like(content).to(DEVICE)\n",
    "\n",
    "\n",
    "    cnn = models.vgg19(pretrained=True).features.to(DEVICE).eval()\n",
    "    cnn_norm_mean = torch.tensor([0.485,0.456,0.406]).to(DEVICE)\n",
    "    cnn_norm_std = torch.tensor([0.229,0.224,0.225]).to(DEVICE)\n",
    "\n",
    "\n",
    "    output = run_style_transfer_pytorch(cnn, cnn_norm_mean, cnn_norm_std, content, style, input_img,\n",
    "        num_steps=num_steps, style_weight=style_weight,\n",
    "        content_weight=content_weight, tv_weight=tv_weight,\n",
    "        style_layer_weights=style_layer_weights)\n",
    "    if preserve_color: output = preserve_color_np(content, output)\n",
    "    tensor_to_pil(output).save(out_path)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1369171",
   "metadata": {},
   "source": [
    "*Ejecución*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8559b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando portrait1.jpg con vangogh.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cesar\\anaconda3\\envs\\iteso\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\cesar\\anaconda3\\envs\\iteso\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\cesar/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150M/548M [06:17<16:45, 415kB/s]    \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid hash value (expected \"dcbb9e9d\", got \"fe4811e047665480c400cf234975cee16463c3089dbe794be1b1717fb555aced\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m out_name \u001b[38;5;241m=\u001b[39m OUTPUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__styled.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcesando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m con \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mstylize_with_controls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtv_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstyle_layer_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_style_layer_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 11\u001b[0m, in \u001b[0;36mstylize_with_controls\u001b[1;34m(content_path, style_path, out_path, content_weight, style_weight, tv_weight, num_steps, preserve_color, style_layer_weights, init)\u001b[0m\n\u001b[0;32m      7\u001b[0m style \u001b[38;5;241m=\u001b[39m image_to_tensor(load_image_pil(style_path, IMG_SIZE))\n\u001b[0;32m      8\u001b[0m input_img \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mif\u001b[39;00m init\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandn_like(content)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 11\u001b[0m cnn \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvgg19\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mto(DEVICE)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     12\u001b[0m cnn_norm_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.485\u001b[39m,\u001b[38;5;241m0.456\u001b[39m,\u001b[38;5;241m0.406\u001b[39m])\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     13\u001b[0m cnn_norm_std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.229\u001b[39m,\u001b[38;5;241m0.224\u001b[39m,\u001b[38;5;241m0.225\u001b[39m])\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\cesar\\anaconda3\\envs\\iteso\\lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cesar\\anaconda3\\envs\\iteso\\lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cesar\\anaconda3\\envs\\iteso\\lib\\site-packages\\torchvision\\models\\vgg.py:485\u001b[0m, in \u001b[0;36mvgg19\u001b[1;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"VGG-19 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m weights \u001b[38;5;241m=\u001b[39m VGG19_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _vgg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, weights, progress, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cesar\\anaconda3\\envs\\iteso\\lib\\site-packages\\torchvision\\models\\vgg.py:105\u001b[0m, in \u001b[0;36m_vgg\u001b[1;34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m model \u001b[38;5;241m=\u001b[39m VGG(make_layers(cfgs[cfg], batch_norm\u001b[38;5;241m=\u001b[39mbatch_norm), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\cesar\\anaconda3\\envs\\iteso\\lib\\site-packages\\torchvision\\models\\_api.py:91\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_state_dict_from_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cesar\\anaconda3\\envs\\iteso\\lib\\site-packages\\torch\\hub.py:871\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[0;32m    869\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[0;32m    870\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 871\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "File \u001b[1;32mc:\\Users\\cesar\\anaconda3\\envs\\iteso\\lib\\site-packages\\torch\\hub.py:760\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    758\u001b[0m         digest \u001b[38;5;241m=\u001b[39m sha256\u001b[38;5;241m.\u001b[39mhexdigest()  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[0;32m    759\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m digest[: \u001b[38;5;28mlen\u001b[39m(hash_prefix)] \u001b[38;5;241m!=\u001b[39m hash_prefix:\n\u001b[1;32m--> 760\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    761\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid hash value (expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhash_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdigest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    762\u001b[0m             )\n\u001b[0;32m    763\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mmove(f\u001b[38;5;241m.\u001b[39mname, dst)\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid hash value (expected \"dcbb9e9d\", got \"fe4811e047665480c400cf234975cee16463c3089dbe794be1b1717fb555aced\")"
     ]
    }
   ],
   "source": [
    "example_style_layer_weights = {'conv_1':1.5, 'conv_2':1.2, 'conv_3':0.8, 'conv_4':0.5, 'conv_5':0.3}\n",
    "\n",
    "content_files = sorted(list(CONTENT_DIR.glob('*.jpg')) + list(CONTENT_DIR.glob('*.png')))[:5]\n",
    "style_files = sorted(list(STYLE_DIR.glob('*.jpg')) + list(STYLE_DIR.glob('*.png')))[:5]\n",
    "\n",
    "\n",
    "for c in content_files:\n",
    "    for s in style_files:\n",
    "        out_name = OUTPUT_DIR / f\"{c.stem}__{s.stem}__styled.jpg\"\n",
    "        print(f'Procesando {c.name} con {s.name}')\n",
    "        stylize_with_controls(str(c), str(s), str(out_name),\n",
    "            content_weight=1e5, style_weight=1e10,\n",
    "            tv_weight=1e-6, num_steps=200,\n",
    "            preserve_color=True,\n",
    "            style_layer_weights=example_style_layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec95bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = stylize_with_controls(\n",
    "    content_path='images/content/portrait1.jpg',\n",
    "    style_path='images/style/vangogh.jpg',\n",
    "    out_path='outputs/test_output.jpg',\n",
    "    content_weight=1e5,       # control de cuánto prevalece el contenido\n",
    "    style_weight=1e10,        # control de cuánto prevalece el estilo\n",
    "    tv_weight=1e-6,           # suavizado de textura\n",
    "    num_steps=50,             # número de iteraciones para prueba rápida\n",
    "    preserve_color=True,      # mantener colores del contenido\n",
    "    style_layer_weights={'conv_1':1.5,'conv_2':1.2,'conv_3':0.8,'conv_4':0.5,'conv_5':0.3},\n",
    "    init='content'            # iniciar con la imagen de contenido\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('outputs/test_output.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iteso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
